# -*- coding: utf-8 -*-
"""Copy of Fake News.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10-pRmE69jo-hAhv2j5eUm5qgz39ApBKC
"""

!pip install transformers

import re

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
df1 = pd.read_csv('/content/drive/MyDrive/Dataset for Colab/ZeroShot/BuzzFeed_fake_news_content.csv')
df2 = pd.read_csv('/content/drive/MyDrive/Dataset for Colab/ZeroShot/BuzzFeed_real_news_content.csv')

df_all = pd.concat([df1, df2], ignore_index=True)

df_all.head()

def preprocess_text_dataset(df, text_column='text', new_column='clean_text'):
    """
    Preprocess text by:
    - Removing punctuation
    - Removing special characters
    - Lowercasing all text
    - Removing extra spaces
    - Preserving original row structure
    """
    def clean_text(text):
        if pd.isnull(text):
            return ""
        text = re.sub(r'[^a-zA-Z0-9\s]', '', text)

        # Lowercase the text
        text = text.lower()

        # Remove extra spaces
        text = re.sub(r'\s+', ' ', text).strip()

        return text

    df[new_column] = df[text_column].apply(clean_text)
    return df

df = preprocess_text_dataset(df_all)
df = df[['title', 'text']]

df.head()

!pip install pytextrank

!python -m spacy download en_core_web_lg

from tqdm import tqdm

import spacy
import pytextrank

# Load spaCy model and add TextRank
nlp = spacy.load("en_core_web_lg")
nlp.add_pipe("textrank", last=True)

def summarize_text(text, limit_phrases=2, limit_sentences=2):
    """
    Summarize input text using spaCy + TextRank.
    """
    if pd.isnull(text) or not isinstance(text, str) or text.strip() == "":
        return ""  # Handle empty or invalid text

    doc = nlp(text)
    summary_sentences = [sent.text for sent in doc._.textrank.summary(
        limit_phrases=limit_phrases,
        limit_sentences=limit_sentences
    )]
    return " ".join(summary_sentences)

tqdm.pandas()  # Initialize tqdm with pandas integration

def add_summary_column(df, text_column='text', summary_column='summary'):
    """
    Apply summarization to each row in the text column and add it as a new column with a progress bar.
    """
    df[summary_column] = df[text_column].progress_apply(summarize_text)
    return df

#df = df.iloc[:37]

df = add_summary_column(df, text_column='text', summary_column='summary')

df = df[['title', 'text','summary']]

df.head()

from transformers import pipeline

# Load model
classifier = pipeline("zero-shot-classification",
                      model="MoritzLaurer/mDeBERTa-v3-base-mnli-xnli",
                      tokenizer="MoritzLaurer/mDeBERTa-v3-base-mnli-xnli",
                      device=0
                      )

df = df[df['summary'].notna()].copy()  # Make a real copy to avoid the warning
df.loc[:, 'summary'] = df['summary'].astype(str)  # Safe way to modify a column

# Prepare list for new dataset
bert_results = []

# Loop over DataFrame
for text in tqdm(df.summary, desc="Classifying Articles"):
    output = classifier(text, candidate_labels=['fake', 'real'])

    # Extract top label and its score
    top_label = output['labels'][0]
    top_score = output['scores'][0]

    # Append to results with order number
    bert_results.append({
        "news_title": text,
        "bert_label": output['labels'][0],
        "bert_score": output['scores'][0]
    })

bert_df = pd.DataFrame(bert_results)
print(bert_df.head())

bert_df

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, mean_absolute_error, confusion_matrix
from sklearn.preprocessing import LabelEncoder

bert_df["true_label"] = df_all["id"].str.extract(r'^(Real|Fake)', expand=False).str.lower()

all_labels = list(set(bert_df["true_label"]).union(set(bert_df["bert_label"])))

le = LabelEncoder()
le.fit(all_labels)

y_true = le.transform(bert_df["true_label"])
y_pred = le.transform(bert_df["bert_label"])

bert_df

# Calculate metrics
acc = accuracy_score(y_true, y_pred)
prec = precision_score(y_true, y_pred)
rec = recall_score(y_true, y_pred)
f1 = f1_score(y_true, y_pred)
mae = mean_absolute_error(y_true, y_pred)
conf_matrix = confusion_matrix(y_true, y_pred)

# Print results
print(f"Accuracy: {acc:.4f}%")
print(f"Precision: {prec:.4f}")
print(f"Recall: {rec:.4f}")
print(f"F1-Score: {f1:.4f}")
print(f"MAE: {mae:.4f}")
print("Confusion Matrix:")
print(conf_matrix)



classifier2 = pipeline("zero-shot-classification",
                      model="facebook/bart-large-mnli",
                      tokenizer="facebook/bart-large-mnli",
                      device=0)

# Prepare list for new dataset
bart_large_results = []

# Loop over DataFrame
for text in tqdm(df.summary, desc="Classifying Articles"):
    output = classifier2(text, candidate_labels=['fake', 'real'])

    # Extract top label and its score
    top_label = output['labels'][0]
    top_score = output['scores'][0]

    # Append to results with order number
    bart_large_results.append({
        "news_title": text,
        "bart_large_label": output['labels'][0],
        "bart_large_score": output['scores'][0]
    })

bart_large_df = pd.DataFrame(bart_large_results)
print(bart_large_df.head())

bart_large_df["true_label"] = df_all["id"].str.extract(r'^(Real|Fake)', expand=False).str.lower()

all_labels = list(set(bart_large_df["true_label"]).union(set(bart_large_df["bart_large_label"])))

le = LabelEncoder()
le.fit(all_labels)

y_true = le.transform(bart_large_df["true_label"])
y_pred = le.transform(bart_large_df["bart_large_label"])

# Calculate metrics
acc = accuracy_score(y_true, y_pred)
prec = precision_score(y_true, y_pred)
rec = recall_score(y_true, y_pred)
f1 = f1_score(y_true, y_pred)
mae = mean_absolute_error(y_true, y_pred)
conf_matrix = confusion_matrix(y_true, y_pred)

# Print results
print(f"Accuracy: {acc:.4f}%")
print(f"Precision: {prec:.4f}")
print(f"Recall: {rec:.4f}")
print(f"F1-Score: {f1:.4f}")
print(f"MAE: {mae:.4f}")
print("Confusion Matrix:")
print(conf_matrix)

classifier3 = pipeline("zero-shot-classification",
                      model="MoritzLaurer/ModernBERT-large-zeroshot-v2.0",
                      tokenizer="MoritzLaurer/ModernBERT-large-zeroshot-v2.0",
                      device=0)

# Prepare list for new dataset
Modernbert_results = []

# Loop over DataFrame
for text in tqdm(df.summary, desc="Classifying Articles"):
    output = classifier3(text, candidate_labels=['fake', 'real'])

    # Extract top label and its score
    top_label = output['labels'][0]
    top_score = output['scores'][0]

    # Append to results with order number
    Modernbert_results.append({
        "news_title": text,
        "Modernbert_label": output['labels'][0],
        "Modernbert_score": output['scores'][0]
    })

Modernbert_df = pd.DataFrame(Modernbert_results)
print(Modernbert_df.head())

Modernbert_df["true_label"] = df_all["id"].str.extract(r'^(Real|Fake)', expand=False).str.lower()

all_labels = list(set(Modernbert_df["true_label"]).union(set(Modernbert_df["Modernbert_label"])))

le = LabelEncoder()
le.fit(all_labels)

y_true = le.transform(Modernbert_df["true_label"])
y_pred = le.transform(Modernbert_df["Modernbert_label"])

# Calculate metrics
acc = accuracy_score(y_true, y_pred)*100
prec = precision_score(y_true, y_pred)
rec = recall_score(y_true, y_pred)
f1 = f1_score(y_true, y_pred)
mae = mean_absolute_error(y_true, y_pred)
conf_matrix = confusion_matrix(y_true, y_pred)

# Print results
print(f"Accuracy: {acc:.4f%}")
print(f"Precision: {prec:.4f}")
print(f"Recall: {rec:.4f}")
print(f"F1-Score: {f1:.4f}")
print(f"MAE: {mae:.4f}")
print("Confusion Matrix:")
print(conf_matrix)

import matplotlib.pyplot as plt
import seaborn as sns

ensemble_results = []
for i in range(len(bert_df)):
    label_scores = {}

    # mDeBERTa
    m_label = bert_df['bert_label'][i]
    m_score = bert_df['bert_score'][i]
    if pd.notna(m_label) and pd.notna(m_score):
        label_scores[m_label] = label_scores.get(m_label, 0) + m_score

    # BART-large
    b_label = bart_large_df['bart_large_label'][i]
    b_score = bart_large_df['bart_large_score'][i]
    if pd.notna(b_label) and pd.notna(b_score):
        label_scores[b_label] = label_scores.get(b_label, 0) + b_score

    # ModernBERT
    mb_label = Modernbert_df['Modernbert_label'][i]
    mb_score = Modernbert_df['Modernbert_score'][i] if 'Modernbert_score' in Modernbert_df.columns else 1.0
    # if you don't have a confidence score for ModernBERT, you can assign a default score like 1.0
    if pd.notna(mb_label) and pd.notna(mb_score):
        label_scores[mb_label] = label_scores.get(mb_label, 0) + mb_score

    if label_scores:
        ensemble_label = max(label_scores, key=lambda k: label_scores[k])
    else:
        # fallback if no scores exist
        ensemble_label = (
            m_label if pd.notna(m_label) else
            (b_label if pd.notna(b_label) else
             (mb_label if pd.notna(mb_label) else None))
        )

    ensemble_results.append({
        "ensemble_label": ensemble_label,
        "true_label": bert_df["true_label"][i],  # assuming all share same true_label order
    })

ensemble_df = pd.DataFrame(ensemble_results)

# Update all_labels including ModernBERT labels
all_labels = set(bert_df['bert_label']) \
    .union(bart_large_df['bart_large_label']) \
    .union(Modernbert_df['Modernbert_label']) \
    .union(ensemble_df['ensemble_label'])

le = LabelEncoder()
le.fit(list(all_labels))

def get_metrics(true_labels, pred_labels):
    y_true = le.transform(true_labels)
    y_pred = le.transform(pred_labels)
    return {
        "accuracy": accuracy_score(y_true, y_pred),
        "precision": precision_score(y_true, y_pred, average='weighted', zero_division=0),
        "recall": recall_score(y_true, y_pred, average='weighted', zero_division=0),
        "f1": f1_score(y_true, y_pred, average='weighted', zero_division=0),
        "mae": mean_absolute_error(y_true, y_pred),
    }

# Calculate individual model scores
mdeberta_scores = get_metrics(bert_df["true_label"], bert_df["bert_label"])
bart_scores = get_metrics(bart_large_df["true_label"], bart_large_df["bart_large_label"])
modernbert_scores = get_metrics(Modernbert_df["true_label"], Modernbert_df["Modernbert_label"])
ensemble_scores = get_metrics(ensemble_df["true_label"], ensemble_df["ensemble_label"])

# Combine scores in one dataframe
ensemble_df = pd.DataFrame({
    "mDeBERTa": mdeberta_scores,
    "BART": bart_scores,
    "ModernBERT": modernbert_scores,
    "Soft Voting": ensemble_scores,
}).T

ensemble_df

sns.set_style("whitegrid")
plt.rcParams.update({
    "axes.facecolor": "#FAFAFA",
    "figure.facecolor": "#FAFAFA",
    "axes.edgecolor": "#DDDDDD",
    "axes.labelcolor": "#333333",
    "xtick.color": "#333333",
    "ytick.color": "#333333",
    "grid.color": "#EEEEEE",
    "font.size": 12
})

# Your dreamy pastel color palette with more contrast
dreamy_colors = sns.color_palette([
    "#7A9CFF",  # deeper blue
    "#7FDBFF",  # brighter cyan
    "#FF8BAA",  # stronger warm pink
    "#FF5C6A",  # vibrant coral
    "#58C79F",  # richer teal-green
    "#FF9B53"   # warm orange
])

# Prepare data
ensemble_df = ensemble_df.fillna(0).astype(float)

# Use dreamy_colors but limit to number of bars/series (usually columns)
palette = dreamy_colors[:len(ensemble_df.columns)]

### Bar Plot ###
plt.figure(figsize=(4, 4))
ensemble_df.plot(kind='bar', color=palette, edgecolor='gray', linewidth=0.7)
plt.title('Model Performance - Bar Plot (Including MAE)', fontsize=16)
plt.ylabel('Score')
plt.ylim(0, 1.05)
plt.xticks(rotation=0)
plt.grid(axis='y', linestyle='--', alpha=0.7)
plt.legend(title="Metrics", loc='upper right')
plt.tight_layout()
#plt.savefig("model_performance_bar_plot.png", dpi=300, bbox_inches='tight')
plt.show()

### Radar Chart ###
labels = ensemble_df.columns.tolist()
angles = np.linspace(0, 2 * np.pi, len(labels), endpoint=False).tolist()
labels += labels[:1]
angles += angles[:1]

plt.figure(figsize=(8, 7))
for idx, model in enumerate(ensemble_df.index):
    values = ensemble_df.loc[model].tolist()
    values += values[:1]
    plt.polar(angles, values, label=model, color=dreamy_colors[idx], linewidth=2)
    plt.fill(angles, values, alpha=0.15, color=dreamy_colors[idx])

plt.thetagrids(np.degrees(angles[:-1]), labels[:-1])
plt.title('Radar Chart of Model Performance', fontsize=14)
plt.legend(loc='upper right', bbox_to_anchor=(1.2, 1.1))
plt.grid(True)
plt.tight_layout()
#plt.savefig("model_performance_radar_chart.svg", dpi=300, bbox_inches='tight')
plt.show()

### Box Plot ###
plt.figure(figsize=(8, 5))
sns.boxplot(data=ensemble_df, palette=palette)
plt.title('Box Plot of Model Metrics', fontsize=16)
plt.ylabel('Score')
plt.tight_layout()
#plt.savefig("model_performance_box_plot.svg", dpi=300, bbox_inches='tight')
plt.show()

### Heatmap ###
plt.figure(figsize=(8, 6))
sns.heatmap(ensemble_df, annot=True, cmap=sns.light_palette("#7A9CFF", as_cmap=True),
            fmt=".2f", cbar_kws={"shrink": 0.8})
plt.title('Heatmap of Model Performance', fontsize=16)
plt.tight_layout()
#plt.savefig("model_performance_heatmap.svg", dpi=300, bbox_inches='tight')
plt.show()